{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble_Methods.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMi/SnDVrV97QzF0cgq8AI8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gorogoro-uk/Machine-Learning/blob/master/Ensemble_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daWg9vOvVdDb"
      },
      "source": [
        "# Ensemble Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AA1sbCqVdHI"
      },
      "source": [
        "### Bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E3wdpJWVdKJ"
      },
      "source": [
        "Using random samples, with replacement, drawn from the training dataset.\n",
        "Final predicition is a sum or average of each individual predictions.\n",
        "Can use a sample of features as well.\n",
        "\n",
        "\n",
        "Benefit is to reduce variance compared to a single estimator.\n",
        "\n",
        "\n",
        "Useful when the classifer is complex (c/f boosting using simple classifiers)\n",
        "\n",
        "https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVKbEdYRXjoo"
      },
      "source": [
        "Basic Usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcYXYW9_XitN",
        "outputId": "5a050341-816e-48c1-e77b-ae60283720e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# example; decision tree bagging\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# prepare dataset\n",
        "X, y = make_classification(n_samples=100, n_features=4,\n",
        "                           n_informative=2, n_redundant=0,\n",
        "                           random_state=0, shuffle=False)\n",
        "\n",
        "# make bagging classifier using decision tree and fit\n",
        "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                         n_estimators=10, random_state=0).fit(X, y)\n",
        "\n",
        "# infer \n",
        "clf.predict([[0, 0, 0, 0]])\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJJ2EmH_VlEC"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5eQa61lba0N"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4femtjJPVrJN"
      },
      "source": [
        "Create multiple decision trees. Each tree is based on a random sample with replacement (bootstrap sample) and a random number of features.\n",
        "\n",
        "Result is average prediction of all the trees.\n",
        "\n",
        "Reduces variance (overfitting).\n",
        "Will not always select the strongest feature resulting in less correlated trees.\n",
        "\n",
        "Hyperparameters:\n",
        "number of estimators & number of features\n",
        "use sqrt(no features) as a start\n",
        "results witll plareacu after a certain number of estimators.\n",
        "\n",
        "Allows for parallelization of computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqaf3rbFbc8X"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNkBLHZpYz3_"
      },
      "source": [
        "# \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X = [[0, 0], [1, 1]]\n",
        "Y = [0, 1]\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=10)\n",
        "\n",
        "clf = clf.fit(X, Y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWcaXDaMVlIB"
      },
      "source": [
        "### Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuE6UTlTVcJ3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}